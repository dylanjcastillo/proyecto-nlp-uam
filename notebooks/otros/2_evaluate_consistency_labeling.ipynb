{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dcast/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "DATA_PATH = \"../../data/\"\n",
    "\n",
    "target = \"target_layer_3\"\n",
    "label = \"resultados\"\n",
    "n_splits = 5\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_orig = pd.read_json(DATA_PATH + \"all.json\").reset_index(drop=True)\n",
    "\n",
    "df_fomento = pd.read_json(\n",
    "    DATA_PATH + \"additional_fomento_de_la_participacion.json\"\n",
    ").reset_index(drop=True)\n",
    "df_colab = pd.read_json(\n",
    "    DATA_PATH + \"additional_colaboracion_ciudadana.json\"\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_unlabeled = pd.read_json(DATA_PATH + \"tweets_sample.json\")\n",
    "df_validated_labels = pd.read_json(DATA_PATH + \"all_labels_20220927.json\").reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset with multi-class targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_base = [\n",
    "    \"transparencia\",\n",
    "    \"colaboracion_ciudadana\",\n",
    "    \"fomento_de_la_participacion\",\n",
    "]\n",
    "layer_2_transparencia = [\n",
    "    \"agradecimientos_y_actos_simbolicos\",\n",
    "    \"decisiones\",\n",
    "    \"estado_de_servicio\",\n",
    "]\n",
    "layer_3_decisiones = [\"racionalidad\", \"contenido\", \"resultados\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = df_unlabeled[df_labels_orig.columns[:-1]].merge(\n",
    "    df_validated_labels[[\"id_tweet\", \"label\"]], on=\"id_tweet\", how=\"inner\"\n",
    ")\n",
    "\n",
    "assert ~df_updated.id_tweet.isin(\n",
    "    df_labels_orig.id_tweet\n",
    ").all()  # Updated tweets are not in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_layers(df):\n",
    "    df[\"target_layer_1\"] = (\n",
    "        df.label.astype(str).str.findall(\"|\".join(layer_1_base)).str[0]\n",
    "    )\n",
    "    df[\"target_layer_2\"] = (\n",
    "        df.label.astype(str).str.findall(\"|\".join(layer_2_transparencia)).str[0]\n",
    "    )\n",
    "    df[\"target_layer_3\"] = (\n",
    "        df.label.astype(str).str.findall(\"|\".join(layer_3_decisiones)).str[0]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.concat([df_labels_orig, df_updated]).reset_index(drop=True)\n",
    "df = generate_target_layers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_processed_df(df_proc, ids_to_exclude, label):\n",
    "    df_proc_ = df_proc[\n",
    "        (df_proc.label.str[0] == label) & ~(df_proc.id_tweet.isin(ids_to_exclude))\n",
    "    ].copy()\n",
    "    df_proc_[\"target_layer_1\"] = df_proc_.label.str[0]\n",
    "    df_proc_[\"target_layer_2\"] = np.nan\n",
    "    df_proc_[\"target_layer_3\"] = np.nan\n",
    "    return df_proc_\n",
    "\n",
    "\n",
    "df_fomento_reduc = generate_processed_df(\n",
    "    df_fomento, df.id_tweet, \"fomento_de_la_participacion\"\n",
    ")\n",
    "\n",
    "wrong_ids_colab = [\n",
    "    3015,\n",
    "    3823,\n",
    "    10551,\n",
    "    9041,\n",
    "    11775,\n",
    "    11181,\n",
    "    9354,\n",
    "]  # Email thread: Etiquetado adicional\n",
    "df_colab_reduc = generate_processed_df(\n",
    "    df_colab,\n",
    "    np.concatenate([df.id_tweet, df_fomento_reduc.id_tweet, wrong_ids_colab]),\n",
    "    \"colaboracion_ciudadana\",\n",
    ")\n",
    "\n",
    "assert ~df_fomento_reduc.link.isin(df.link).any()\n",
    "assert ~df_colab_reduc.link.isin(df.link).any()\n",
    "assert ~df_colab_reduc.link.isin(df_fomento_reduc.link).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 30), (2100, 30), (93, 43), (74, 43), (3100, 33))"
      ]
     },
     "execution_count": 1291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = df.copy()\n",
    "df_labels_orig.shape, df_updated.shape, df_fomento_reduc.shape, df_colab_reduc.shape, df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeling_1 = df_output[:999][[\"text\", \"target_layer_1\", \"target_layer_2\", \"target_layer_3\"]].copy()\n",
    "df_labeling_2 = df_output[999:2098][[\"text\", \"target_layer_1\", \"target_layer_2\", \"target_layer_3\"]].copy()\n",
    "df_labeling_3 = df_output[2098:][[\"text\", \"target_layer_1\", \"target_layer_2\", \"target_layer_3\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target_layer_3</th>\n",
       "      <th>resultados</th>\n",
       "      <th>contenido</th>\n",
       "      <th>racionalidad</th>\n",
       "      <th>labeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.205273</td>\n",
       "      <td>0.128060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.722496</td>\n",
       "      <td>0.169130</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.717996</td>\n",
       "      <td>0.200371</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target_layer_3  resultados  contenido  racionalidad  labeling\n",
       "0                 0.666667   0.205273      0.128060         0\n",
       "0                 0.722496   0.169130      0.108374         1\n",
       "0                 0.717996   0.200371      0.081633         2"
      ]
     },
     "execution_count": 1293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for i, df_ in enumerate([df_labeling_1, df_labeling_2, df_labeling_3]):\n",
    "    dfs.append(\n",
    "        pd.DataFrame(df_.value_counts(target, normalize=True))\n",
    "        .T.assign(labeling=i)\n",
    "    )\n",
    "pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labeling_df(target, label):\n",
    "    return pd.concat(\n",
    "        [\n",
    "            df_labeling_1.query(f\"{target} == '{label}'\").assign(labeling=1),\n",
    "            df_labeling_2.query(f\"{target} == '{label}'\").assign(labeling=2),\n",
    "            df_labeling_3.query(f\"{target} == '{label}'\").assign(labeling=3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df_labeling_comp = generate_labeling_df(target, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \" \", text\n",
    "    )\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df_labeling_comp[\"clean_text\"] = df_labeling_comp.text.map(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text, remove_stopwords=True):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(\n",
    "            f\"[{re.escape(string.punctuation)}]\", \" \", text\n",
    "        )\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        if remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "def find_most_common(df_, n=100, ngr=1):\n",
    "    if ngr == 1:\n",
    "        tokens = nltk.word_tokenize(\n",
    "            \"\\n\".join(df_.query(f\"{target} == '{label}'\").text.map(prepare_text).tolist()),\n",
    "            language=\"spanish\",\n",
    "        )\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(\n",
    "            \"\\n\".join(df_.query(f\"{target} == '{label}'\").text.map(lambda x: prepare_text(x, True)).tolist()),\n",
    "            language=\"spanish\",\n",
    "        )\n",
    "    return Counter(ngrams(tokens, ngr)).most_common(n)\n",
    "\n",
    "for n in [1, 2]:\n",
    "    pd.DataFrame(\n",
    "        find_most_common(df_labeling_1, ngr=n)\n",
    "    ).rename(columns={0: \"word\", 1: f\"{label}_1\"}).merge(\n",
    "        pd.DataFrame(\n",
    "            find_most_common(df_labeling_2, ngr=n)\n",
    "        ).rename(columns={0: \"word\", 1: f\"{label}_2\"}),\n",
    "        on=\"word\",\n",
    "        how=\"outer\"\n",
    "    ).merge(\n",
    "        pd.DataFrame(\n",
    "            find_most_common(df_labeling_3, ngr=n)\n",
    "        ).rename(columns={0: \"word\", 1: f\"{label}_3\"}),\n",
    "        on=\"word\",\n",
    "        how=\"outer\"\n",
    "    ).to_excel(DATA_PATH + f\"drift/most_common_{label}_{n}gram.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0: 944 observations for training / 237 observations for validation\n",
      "Split 1: 945 observations for training / 236 observations for validation\n",
      "Split 2: 945 observations for training / 236 observations for validation\n",
      "Split 3: 945 observations for training / 236 observations for validation\n",
      "Split 4: 945 observations for training / 236 observations for validation\n",
      "\n",
      "Classification report (last fold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.25      0.30        71\n",
      "           2       0.39      0.53      0.45        88\n",
      "           3       0.35      0.30      0.32        77\n",
      "\n",
      "    accuracy                           0.37       236\n",
      "   macro avg       0.37      0.36      0.36       236\n",
      "weighted avg       0.37      0.37      0.36       236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accs_diff = []\n",
    "df_labeling_comp = df_labeling_comp.sample(frac=1).reset_index(drop=True)\n",
    "df_labeling_comp[\"kfold\"] = -1\n",
    "\n",
    "kf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "for f, (t, v) in enumerate(kf.split(X=df_labeling_comp, y=df_labeling_comp.labeling)):\n",
    "    df_labeling_comp.loc[v, \"kfold\"] = f\n",
    "\n",
    "for f in range(n_splits):\n",
    "    df_train = df_labeling_comp[df_labeling_comp.kfold != f].reset_index(drop=True)\n",
    "    df_val = df_labeling_comp[df_labeling_comp.kfold == f].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Split {f}: {df_train.shape[0]} observations for training / {df_val.shape[0]} observations for validation\")\n",
    "    vec = CountVectorizer(\n",
    "        ngram_range=(1, 3), \n",
    "        stop_words=\"english\",\n",
    "    )\n",
    "    svd = TruncatedSVD(n_components=120)\n",
    "\n",
    "    X_count = vec.fit_transform(df_train.clean_text)\n",
    "    X_train = svd.fit_transform(X_count)\n",
    "    X_val = svd.transform(vec.transform(df_val.clean_text))\n",
    "\n",
    "    y_train = df_train.labeling\n",
    "    y_val = df_val.labeling\n",
    "\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    accs_diff.append(\n",
    "        accuracy_score(y_val, preds) - df_val.labeling.value_counts(normalize=True).max()\n",
    "    )\n",
    "\n",
    "print(\"\\nClassification report (last fold):\")\n",
    "print(classification_report(y_val, preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04219409282700426,\n",
       " -0.01694915254237289,\n",
       " -0.008474576271186474,\n",
       " -0.1059322033898305,\n",
       " 0.0]"
      ]
     },
     "execution_count": 1298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy delta: -0.03 +/- 0.04\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean accuracy delta: {np.mean(accs_diff):.2f} +/- {np.std(accs_diff):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45997051a618164a2aaa1103953d009f53296a5c7fd869ab5dc2d414330730df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
