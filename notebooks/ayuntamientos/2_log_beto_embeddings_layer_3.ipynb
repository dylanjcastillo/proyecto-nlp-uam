{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "R_q-Nu53y4IT"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet sentence-transformers emoji"
      ],
      "id": "R_q-Nu53y4IT"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrQ-eZ5-zoyi",
        "outputId": "25ccbf4b-3b65-41da-f5be-59396e238ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    DATA_PATH = \"/content/drive/MyDrive/nlp-tweets-classification/\"\n",
        "    ARTIFACTS_PATH = \"/content/drive/MyDrive/nlp-tweets-classification/\"\n",
        "except ModuleNotFoundError:\n",
        "    DATA_PATH = \"../../data/\"\n",
        "    ARTIFACTS_PATH = \"../../artifacts/\"\n",
        "    running_in_colab = False\n"
      ],
      "id": "SrQ-eZ5-zoyi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25e42212"
      },
      "source": [
        "## Imports\n"
      ],
      "id": "25e42212"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "bQ_E89Xbzlk7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import random\n",
        "import uuid\n",
        "\n",
        "from emoji import demojize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    silhouette_samples,\n",
        "    silhouette_score,\n",
        "    cohen_kappa_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    average_precision_score,\n",
        "    f1_score,\n",
        "    balanced_accuracy_score,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    StratifiedKFold,\n",
        "    GridSearchCV,\n",
        "    StratifiedShuffleSplit,\n",
        ")\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier, XGBRFClassifier\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n"
      ],
      "id": "bQ_E89Xbzlk7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ac68cd"
      },
      "source": [
        "## Utility functions\n"
      ],
      "id": "54ac68cd"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "9928c2c4"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if \"http\" not in t]\n",
        "    tokens = [demojize(t, language=\"es\") for t in tokens]\n",
        "    tokens = [t.replace(\"@\", \"\") for t in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def get_hyperparams_space(model_name):\n",
        "    if model_name == \"log\":\n",
        "        hyperparams = dict(\n",
        "            C=[0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            penalty=[\"l1\", \"l2\"],\n",
        "            class_weight=[\"balanced\", None],\n",
        "        )\n",
        "    elif model_name == \"rf\":\n",
        "        hyperparams = dict(\n",
        "            n_estimators=[120, 500, 1200],\n",
        "            max_depth=[5, 8, 15, 25],\n",
        "            class_weight=[\"balanced\", None],\n",
        "        )\n",
        "    elif model_name == \"svc\":\n",
        "        hyperparams = dict(\n",
        "            C=[0.001, 0.01, 0.1, 1, 10, 100], class_weight=[\"balanced\", None]\n",
        "        )\n",
        "    elif model_name == \"xgb\":\n",
        "        hyperparams = dict(\n",
        "            eta=[0.01, 0.05, 0.1],\n",
        "            gamma=[0.1, 0.5, 1],\n",
        "            max_depth=[3, 12, 25],\n",
        "            min_child_weight=[1, 3, 7],\n",
        "            subsample=[0.6, 0.8, 1],\n",
        "            colsample_bytree=[0.6, 0.8, 1],\n",
        "        )\n",
        "    elif model_name == \"nb\":\n",
        "        hyperparams = dict()\n",
        "    elif model_name == \"lgbm\":\n",
        "        hyperparams = dict(\n",
        "            learning_rate=[0.01, 0.05, 0.1],\n",
        "        )\n",
        "    else:\n",
        "        raise Exception(f\"No hyperparams for model {model_name}\")\n",
        "    return hyperparams\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "    if model_name == \"lgbm\":\n",
        "        return LGBMClassifier()\n",
        "    elif model_name == \"hist\":\n",
        "        return HistGradientBoostingClassifier()\n",
        "    elif model_name == \"log\":\n",
        "        return LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\")\n",
        "    elif model_name == \"rf\":\n",
        "        return RandomForestClassifier(\n",
        "            n_jobs=-1,\n",
        "            n_estimators=1200,\n",
        "        )\n",
        "    elif model_name == \"nb\":\n",
        "        return GaussianNB()\n",
        "    elif model_name == \"xgb\":\n",
        "        return XGBClassifier(\n",
        "            tree_method=\"hist\", use_label_encoder=False, max_depth=10, eta=0.1\n",
        "        )\n",
        "    elif model_name == \"svc\":\n",
        "        return LinearSVC(class_weight=\"balanced\")\n",
        "    else:\n",
        "        raise ValueError(format)\n",
        "\n",
        "\n",
        "def get_mode_rows(a):\n",
        "    a = np.ascontiguousarray(a)\n",
        "    void_dt = np.dtype((np.void, a.dtype.itemsize * np.prod(a.shape[1:])))\n",
        "    _, ids, count = np.unique(a.view(void_dt).ravel(), return_index=1, return_counts=1)\n",
        "    largest_count_id = ids[count.argmax()]\n",
        "    most_frequent_row = a[largest_count_id]\n",
        "    return most_frequent_row\n"
      ],
      "id": "9928c2c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "509b840e"
      },
      "source": [
        "## Set notebook parameters\n"
      ],
      "id": "509b840e"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "a580a384"
      },
      "outputs": [],
      "source": [
        "col_target = \"target_layer_3\"\n",
        "dataset_name = \"all_multiclass_20220911.json\"\n",
        "model_name = \"log\"\n",
        "n_splits = 10\n",
        "use_precalculated_embeddings = True \n",
        "run_hyperparams_search = True \n",
        "use_full_dataset = True"
      ],
      "id": "a580a384"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42905b26"
      },
      "source": [
        "## Read data\n"
      ],
      "id": "42905b26"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "1204c89a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(DATA_PATH + dataset_name).dropna(subset=[col_target])\n",
        "df_translated = pd.read_csv(DATA_PATH + \"tweets_traducidos.csv\")\n",
        "df_translated[\"text\"] = df_translated.texto_traducido.combine_first(df_translated.text)\n",
        "\n",
        "if not use_full_dataset:\n",
        "    df_sample = pd.read_json(DATA_PATH + \"tweets_sample.json\")\n",
        "    df_test = df_sample[~df_sample.id.isin(df.id)].reset_index()\n",
        "else:\n",
        "    if col_target == \"target_layer_1\":\n",
        "        df_test = (\n",
        "            pd.read_csv(DATA_PATH + \"tweets_traducidos.csv\")\n",
        "        )\n",
        "        df_test[\"text\"] = df_test.texto_traducido.combine_first(df_test.text)\n",
        "    elif col_target == \"target_layer_2\":\n",
        "        df_test = (\n",
        "            pd.read_csv(DATA_PATH + \"output_target_layer_1.csv\")\n",
        "            .query(\"target_layer_1 == 'transparencia'\")\n",
        "            .reset_index()\n",
        "        )\n",
        "    elif col_target == \"target_layer_3\":\n",
        "        df_test = (\n",
        "            pd.read_csv(DATA_PATH + \"output_target_layer_2.csv\")\n",
        "            .query(\"target_layer_2 == 'decisiones'\")\n",
        "            .reset_index()\n",
        "        )"
      ],
      "id": "1204c89a"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "d34978ea"
      },
      "outputs": [],
      "source": [
        "assert df_translated.link.notna().all()\n",
        "assert df_translated.link.nunique() == df_translated.shape[0]\n"
      ],
      "id": "d34978ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOTvL51j0brB"
      },
      "source": [
        "## Create embeddings\n"
      ],
      "id": "kOTvL51j0brB"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "T_X-xlv70ffu"
      },
      "outputs": [],
      "source": [
        "if use_precalculated_embeddings:\n",
        "    df_embeddings = pd.read_json(DATA_PATH + \"tweets_embeddings.json\")\n",
        "else:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    cleaned_tweets = [clean_text(tweet) for tweet in df_translated.text]\n",
        "\n",
        "    model = SentenceTransformer(\"hiiamsid/sentence_similarity_spanish_es\")\n",
        "    embeddings = model.encode(cleaned_tweets)\n",
        "\n",
        "    df_embeddings = pd.concat([df_translated[\"link\"], pd.DataFrame(embeddings)], axis=1)\n",
        "    df_embeddings.to_json(DATA_PATH + \"tweets_embeddings.json\")\n",
        "\n",
        "assert df.link.isin(df_embeddings.link).all()\n",
        "assert df_translated.link.isin(df_embeddings.link).all()\n"
      ],
      "id": "T_X-xlv70ffu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62a4e6c4"
      },
      "source": [
        "## Train model\n"
      ],
      "id": "62a4e6c4"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "f7448396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14c0847-99da-406c-b4de-bf8c2129012c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log\n",
            "Split 0: 1002 observations for training / 112 observations for validation\n",
            "f1: 0.63, kappa: 0.48\n",
            "Split 1: 1002 observations for training / 112 observations for validation\n",
            "f1: 0.63, kappa: 0.51\n",
            "Split 2: 1002 observations for training / 112 observations for validation\n",
            "f1: 0.63, kappa: 0.48\n",
            "Split 3: 1002 observations for training / 112 observations for validation\n",
            "f1: 0.60, kappa: 0.44\n",
            "Split 4: 1003 observations for training / 111 observations for validation\n",
            "f1: 0.60, kappa: 0.44\n",
            "Split 5: 1003 observations for training / 111 observations for validation\n",
            "f1: 0.62, kappa: 0.45\n",
            "Split 6: 1003 observations for training / 111 observations for validation\n",
            "f1: 0.63, kappa: 0.46\n",
            "Split 7: 1003 observations for training / 111 observations for validation\n",
            "f1: 0.64, kappa: 0.47\n",
            "Split 8: 1003 observations for training / 111 observations for validation\n",
            "f1: 0.64, kappa: 0.48\n",
            "Split 9: 1003 observations for training / 111 observations for validation\n",
            "f1: 0.65, kappa: 0.49\n"
          ]
        }
      ],
      "source": [
        "df_results = pd.DataFrame()\n",
        "\n",
        "x_test = df_test[[\"link\"]].merge(df_embeddings, how=\"inner\", on=\"link\").drop(columns=\"link\")\n",
        "df_label = (\n",
        "    df[[\"link\", col_target]].merge(df_embeddings, how=\"inner\", on=\"link\").drop(columns=\"link\")\n",
        ")\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(df_label[col_target])\n",
        "joblib.dump(le, ARTIFACTS_PATH + f\"{col_target}/label_encoder.joblib\")\n",
        "\n",
        "df_label = df_label.sample(frac=1).reset_index(drop=True).dropna()\n",
        "df_label[\"kfold\"] = -1\n",
        "\n",
        "if n_splits > 1:\n",
        "    kf = StratifiedKFold(n_splits=n_splits)\n",
        "    for f, (t, v) in enumerate(kf.split(X=df_label, y=df_label[col_target])):\n",
        "        df_label.loc[v, \"kfold\"] = f\n",
        "else:\n",
        "    ss = StratifiedShuffleSplit(n_splits=1, test_size=0.15)\n",
        "    f = next(ss.split(X=df_label, y=df_label[col_target]))\n",
        "    t = f[0]\n",
        "    v = f[1]\n",
        "    df_label.loc[v, \"kfold\"] = 0\n",
        "\n",
        "f1_scores = []\n",
        "kappa_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "acc_scores = []\n",
        "test_probas = []\n",
        "l1_f1_scores = []\n",
        "l2_f1_scores = []\n",
        "l3_f1_scores = []\n",
        "l1_precision_scores = []\n",
        "l2_precision_scores = []\n",
        "l3_precision_scores = []\n",
        "l1_recall_scores = []\n",
        "l2_recall_scores = []\n",
        "l3_recall_scores = []\n",
        "\n",
        "print(f\"Training {model_name}\")\n",
        "for f in range(n_splits):\n",
        "    df_train = df_label[df_label.kfold != f].reset_index(drop=True)\n",
        "    df_val = df_label[df_label.kfold == f].reset_index(drop=True)\n",
        "\n",
        "    print(\n",
        "        f\"Split {f}: {df_train.shape[0]} observations for training / {df_val.shape[0]} observations for validation\"\n",
        "    )\n",
        "    x_train = df_train.iloc[:, 1:-1]\n",
        "    x_val = df_val.iloc[:, 1:-1]\n",
        "    y_train = le.transform(df_train[col_target])\n",
        "    y_val = le.transform(df_val[col_target])\n",
        "\n",
        "    if run_hyperparams_search:\n",
        "        space = get_hyperparams_space(model_name)\n",
        "        cv_inner = StratifiedKFold(n_splits=3, shuffle=True)\n",
        "        search = GridSearchCV(\n",
        "            get_model(model_name), space, scoring=\"f1_macro\", cv=cv_inner, refit=True\n",
        "        )\n",
        "        result = search.fit(x_train, y_train)\n",
        "        model = result.best_estimator_\n",
        "    else:\n",
        "        model = get_model(model_name)\n",
        "\n",
        "    joblib.dump(model, ARTIFACTS_PATH + f\"{col_target}/model_{model_name}_{f}.joblib\")\n",
        "\n",
        "    model.fit(x_train, y_train)\n",
        "    preds = model.predict(x_val)\n",
        "    test_probas.append(model.predict_proba(x_test))\n",
        "\n",
        "    f1_ind_scores = f1_score(y_val, preds, average=None)\n",
        "    precision_ind_scores = precision_score(y_val, preds, average=None)\n",
        "    recall_ind_scores = recall_score(y_val, preds, average=None)\n",
        "\n",
        "    l1_f1_scores.append(f1_ind_scores[0])\n",
        "    l2_f1_scores.append(f1_ind_scores[1])\n",
        "    l3_f1_scores.append(f1_ind_scores[2])\n",
        "\n",
        "    l1_precision_scores.append(precision_ind_scores[0])\n",
        "    l2_precision_scores.append(precision_ind_scores[1])\n",
        "    l3_precision_scores.append(precision_ind_scores[2])\n",
        "\n",
        "    l1_recall_scores.append(recall_ind_scores[0])\n",
        "    l2_recall_scores.append(recall_ind_scores[1])\n",
        "    l3_recall_scores.append(recall_ind_scores[2])\n",
        "\n",
        "    f1_scores.append(f1_score(y_val, preds, average=\"macro\"))\n",
        "    kappa_scores.append(cohen_kappa_score(y_val, preds))\n",
        "    precision_scores.append(precision_score(y_val, preds, average=\"macro\"))\n",
        "    recall_scores.append(recall_score(y_val, preds, average=\"macro\"))\n",
        "\n",
        "    # print(classification_report(y_val, preds, target_names=le.classes_))\n",
        "    print(f\"f1: {np.mean(f1_scores):.2f}, kappa: {np.mean(kappa_scores):.2f}\")\n",
        "\n",
        "df_results = pd.concat(\n",
        "    [\n",
        "        df_results,\n",
        "        pd.DataFrame(\n",
        "            dict(\n",
        "                target=col_target,\n",
        "                model=[model_name],\n",
        "                n_splits=n_splits,\n",
        "                f1=np.mean(f1_scores),\n",
        "                kappa=np.mean(kappa_scores),\n",
        "                precision=np.mean(precision_scores),\n",
        "                recall=np.mean(recall_scores),\n",
        "                l1_f1=np.mean(l1_f1_scores),\n",
        "                l2_f1=np.mean(l2_f1_scores),\n",
        "                l3_f1=np.mean(l3_f1_scores),\n",
        "                l1_precision=np.mean(l1_precision_scores),\n",
        "                l2_precision=np.mean(l2_precision_scores),\n",
        "                l3_precision=np.mean(l3_precision_scores),\n",
        "                l1_recall=np.mean(l1_recall_scores),\n",
        "                l2_recall=np.mean(l2_recall_scores),\n",
        "                l3_recall=np.mean(l3_recall_scores),\n",
        "                f1_scores=str([round(s, 2) for s in f1_scores]),\n",
        "                kappa_scores=str([round(s, 2) for s in kappa_scores]),\n",
        "                precision_scores=str([round(s, 2) for s in precision_scores]),\n",
        "                recall_scores=str([round(s, 2) for s in recall_scores]),\n",
        "            )\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "df_results = df_results.rename(\n",
        "    columns={\n",
        "        \"l1_f1\": f\"{le.classes_[0]}_f1\",\n",
        "        \"l2_f1\": f\"{le.classes_[1]}_f1\",\n",
        "        \"l3_f1\": f\"{le.classes_[2]}_f1\",\n",
        "        \"l1_precision\": f\"{le.classes_[0]}_precision\",\n",
        "        \"l2_precision\": f\"{le.classes_[1]}_precision\",\n",
        "        \"l3_precision\": f\"{le.classes_[2]}_precision\",\n",
        "        \"l1_recall\": f\"{le.classes_[0]}_recall\",\n",
        "        \"l2_recall\": f\"{le.classes_[1]}_recall\",\n",
        "        \"l3_recall\": f\"{le.classes_[2]}_recall\",\n",
        "    }\n",
        ")\n"
      ],
      "id": "f7448396"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90b9a2db"
      },
      "source": [
        "## Save results\n"
      ],
      "id": "90b9a2db"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "966fc616"
      },
      "outputs": [],
      "source": [
        "results_path = DATA_PATH + \"results.csv\"\n",
        "if run_hyperparams_search:\n",
        "    results_path = DATA_PATH + \"results_best_hyperparams.csv\"\n",
        "df_results.to_csv(\n",
        "    results_path, mode=\"a\", header=not os.path.exists(results_path), index=False\n",
        ")"
      ],
      "id": "966fc616"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "t6fQ2TY6facL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "a3b228f1-672f-47dc-830d-2e302aba0d7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                   0\n",
              "target                                                                target_layer_3\n",
              "model                                                                            log\n",
              "n_splits                                                                          10\n",
              "f1                                                                          0.648702\n",
              "kappa                                                                       0.493639\n",
              "precision                                                                   0.657647\n",
              "recall                                                                      0.650041\n",
              "contenido_f1                                                                0.544801\n",
              "racionalidad_f1                                                             0.547427\n",
              "resultados_f1                                                               0.853879\n",
              "contenido_precision                                                         0.562803\n",
              "racionalidad_precision                                                      0.560381\n",
              "resultados_precision                                                        0.849756\n",
              "contenido_recall                                                                0.54\n",
              "racionalidad_recall                                                             0.55\n",
              "resultados_recall                                                           0.860123\n",
              "f1_scores                [0.63, 0.62, 0.62, 0.52, 0.6, 0.73, 0.66, 0.71, 0.68, 0.72]\n",
              "kappa_scores             [0.48, 0.54, 0.41, 0.34, 0.4, 0.53, 0.51, 0.54, 0.57, 0.61]\n",
              "precision_scores        [0.68, 0.64, 0.62, 0.53, 0.62, 0.75, 0.66, 0.68, 0.67, 0.73]\n",
              "recall_scores             [0.61, 0.61, 0.66, 0.51, 0.6, 0.72, 0.65, 0.73, 0.7, 0.71]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91ee0ba2-af95-4c66-96c3-0e2711fb3783\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>target_layer_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>n_splits</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1</th>\n",
              "      <td>0.648702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kappa</th>\n",
              "      <td>0.493639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.657647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.650041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contenido_f1</th>\n",
              "      <td>0.544801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>racionalidad_f1</th>\n",
              "      <td>0.547427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resultados_f1</th>\n",
              "      <td>0.853879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contenido_precision</th>\n",
              "      <td>0.562803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>racionalidad_precision</th>\n",
              "      <td>0.560381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resultados_precision</th>\n",
              "      <td>0.849756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>contenido_recall</th>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>racionalidad_recall</th>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>resultados_recall</th>\n",
              "      <td>0.860123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1_scores</th>\n",
              "      <td>[0.63, 0.62, 0.62, 0.52, 0.6, 0.73, 0.66, 0.71, 0.68, 0.72]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kappa_scores</th>\n",
              "      <td>[0.48, 0.54, 0.41, 0.34, 0.4, 0.53, 0.51, 0.54, 0.57, 0.61]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>precision_scores</th>\n",
              "      <td>[0.68, 0.64, 0.62, 0.53, 0.62, 0.75, 0.66, 0.68, 0.67, 0.73]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall_scores</th>\n",
              "      <td>[0.61, 0.61, 0.66, 0.51, 0.6, 0.72, 0.65, 0.73, 0.7, 0.71]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91ee0ba2-af95-4c66-96c3-0e2711fb3783')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91ee0ba2-af95-4c66-96c3-0e2711fb3783 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91ee0ba2-af95-4c66-96c3-0e2711fb3783');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "df_results.T"
      ],
      "id": "t6fQ2TY6facL"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "sviug2NojL07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3077c9-ecf1-40bc-c951-cd59c49e2329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "resultados      0.673978\n",
              "contenido       0.218345\n",
              "racionalidad    0.107677\n",
              "Name: target_layer_3, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "df_train[col_target].value_counts(normalize=True)"
      ],
      "id": "sviug2NojL07"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ic1rnQlulMk8"
      },
      "id": "ic1rnQlulMk8",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "SssN5ylZQP47"
      },
      "outputs": [],
      "source": [
        "col_probas = []\n",
        "for j in range(x_test.shape[0]):\n",
        "  temp_probas = np.array([test_probas[i][j] for i in range(len(test_probas))])\n",
        "  col_probas.append(temp_probas.sum(axis=0) / temp_probas.sum())\n",
        "\n",
        "df_test[col_target] = le.inverse_transform(np.argmax(np.array(col_probas), axis=1))\n",
        "\n",
        "df_probas = pd.DataFrame(col_probas).rename(\n",
        "    columns={\n",
        "        0: \"prob_\" + le.inverse_transform([0])[0],\n",
        "        1: \"prob_\" + le.inverse_transform([1])[0],\n",
        "        2: \"prob_\" + le.inverse_transform([2])[0],\n",
        "    }\n",
        ")\n",
        "\n",
        "# concat df_test with df_counts\n",
        "rows_prev = df_test.shape[0]\n",
        "df_test = pd.concat([df_test, df_probas], axis=1)\n",
        "\n",
        "assert df_test.shape[0] == rows_prev"
      ],
      "id": "SssN5ylZQP47"
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "30533aad"
      },
      "outputs": [],
      "source": [
        "if use_full_dataset:\n",
        "    df_test[[\"id_tweet\", \"link\", \"text\", col_target] + df_probas.columns.tolist()].to_csv(\n",
        "        DATA_PATH + f\"output_{col_target}.csv\",\n",
        "        index=False,\n",
        "    )\n",
        "else:\n",
        "    df_test[[\"id_tweet\", \"link\", \"text\", col_target] + df_probas.columns.tolist()].to_csv(\n",
        "        DATA_PATH + f\"sample_preds_{col_target}.csv\",\n",
        "        index=False,\n",
        "    )"
      ],
      "id": "30533aad"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Tags",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 47.584666,
      "end_time": "2022-08-09T15:27:48.180714",
      "environment_variables": {},
      "exception": null,
      "input_path": "0_models_baseline_bert_embeddings.ipynb",
      "output_path": "papermill/0_models_baseline_bert_embeddings_decisiones_hist.ipynb",
      "parameters": {
        "col_target": "decisiones",
        "model_name": "hist"
      },
      "start_time": "2022-08-09T15:27:00.596048",
      "version": "2.3.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "45997051a618164a2aaa1103953d009f53296a5c7fd869ab5dc2d414330730df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}